{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"\")\n",
    "X = df.drop(\"d33\",axis=1)\n",
    "y = df['d33']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle columns using numpy's random.permutation\n",
    "np.random.seed(0)  # Random seed to ensure reproducible results\n",
    "new_columns = df.columns[np.random.permutation(len(df.columns))]\n",
    "dff = df[new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# CatBoost model parameters\n",
    "params_catboost = {\n",
    "    'learning_rate': 0.02,            # Learning rate, which controls the step size of each step\n",
    "    'loss_function': 'RMSE',          # Loss function, where RMSE is used\n",
    "    'eval_metric': 'RMSE',            # Evaluation metric, where RMSE is used\n",
    "    'random_seed': 42,                # Random seed\n",
    "    'verbose': 100,                   # The detail level of CatBoost's output information and output the progress every 100 iterations\n",
    "    'thread_count': -1,               # Use all available CPU cores\n",
    "    'bootstrap_type': 'Bernoulli',    # Bootstrap type, which is used to control the generalization ability of the model\n",
    "    'subsample': 0.7,                 # The proportion of samples randomly selected in each iteration\n",
    "    'colsample_bylevel': 0.6          # The proportion of features randomly selected at each layer, which is used to increase the generalization ability of the model\n",
    "}\n",
    "\n",
    "# Initialize the CatBoost model\n",
    "model_catboost = cb.CatBoostRegressor(**params_catboost)\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = {\n",
    "    'iterations': [100, 200, 300, 400, 500],  # Number of iterations (equivalent to the number of trees)\n",
    "    'depth': [3, 4, 5, 6, 7],                 # Tree depth\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1], # Learning rate\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for grid search and k-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_catboost,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Evaluation metric is negative MSE\n",
    "    cv=10,                             # 10-fold cross-validation\n",
    "    n_jobs=-1,                         # Parallel computing\n",
    ")\n",
    "\n",
    "# Model training\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the optimal parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best RMSE score: \", (-grid_search.best_score_) ** 0.5)  # Restore RMSE\n",
    "\n",
    "# Train the model with optimal parameters\n",
    "best_model_catboost = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and sort the feature importance of CatBoost\n",
    "catboost_feature_importances = best_model_catboost.get_feature_importance()\n",
    "catboost_sorted_indices = np.argsort(catboost_feature_importances)[::-1]\n",
    "catboost_sorted_features = X_train.columns[catboost_sorted_indices]\n",
    "catboost_sorted_importances = catboost_feature_importances[catboost_sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_model_catboost.feature_importances_\n",
    "\n",
    "# Construct a feature importance ranking\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Take the top 20 important features\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "# Adjust the font size\n",
    "plt.figure(figsize=(12, 8), dpi=1000)  \n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance', fontsize=14)  \n",
    "plt.ylabel('Feature', fontsize=14)  \n",
    "plt.title(f'Top {top_n} Feature Importance', fontsize=16)  \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)  \n",
    "plt.gca().invert_yaxis()  \n",
    "plt.savefig(\"1.png\", format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# LightGBM model parameters\n",
    "params_lgb = {\n",
    "    'boosting_type': 'gbdt',           # Boosting method, here using Gradient Boosting Decision Trees (GBDT)\n",
    "    'objective': 'regression',         # Loss function, here using it for regression tasks\n",
    "    'num_leaves': 127,                 # Number of leaf nodes per tree, which controls model complexity\n",
    "    'verbosity': 1,                    # Control the detail level of LightGBM's output information\n",
    "    'seed': 42,                        # Random seed, used to reproduce model results\n",
    "    'n_jobs': -1,                      # Number of threads for parallel computing, where -1 indicates using all available CPU cores\n",
    "    'colsample_bytree': 0.6,           # Proportion of features randomly selected for each tree, used to enhance the model's generalization ability\n",
    "    'subsample': 0.7,                  # Proportion of samples randomly selected in each iteration, used to enhance the model's generalization ability\n",
    "    'metric': 'rmse'                   # Evaluation metric, here using Root Mean Square Error (RMSE)\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "model_lgb = lgb.LGBMRegressor(**params_lgb)\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Number of trees\n",
    "    'max_depth': [3, 4, 5, 6, 7],               # Tree depth\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1],   # Learning rate\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for grid search and k-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_lgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Evaluation metric is negative MSE\n",
    "    cv=10,                             # 10-fold cross-validation\n",
    "    n_jobs=-1,                         # Parallel computing\n",
    "    verbose=1                          # Output detailed progress information\n",
    ")\n",
    "\n",
    "# Model training\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the optimal parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best RMSE score: \", (-grid_search.best_score_) ** 0.5)  # Restore RMSE\n",
    "\n",
    "\n",
    "# Train the model with optimal parameters\n",
    "best_model_lgb = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and sort the feature importance of LightGBM\n",
    "lgb_feature_importances = best_model_lgb.feature_importances_\n",
    "lgb_sorted_indices = np.argsort(lgb_feature_importances)[::-1]\n",
    "lgb_sorted_features = X_train.columns[lgb_sorted_indices]\n",
    "lgb_sorted_importances = lgb_feature_importances[lgb_sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_model_lgb.feature_importances_\n",
    "\n",
    "# Construct a feature importance ranking\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Take the top 20 important features\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "# Adjust the font size\n",
    "plt.figure(figsize=(12, 8), dpi=1000)  \n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance', fontsize=14)  \n",
    "plt.ylabel('Feature', fontsize=14)  \n",
    "plt.title(f'Top {top_n} Feature Importance', fontsize=16)  \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)  \n",
    "plt.gca().invert_yaxis()  \n",
    "plt.savefig(\"1.png\", format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# RandomForestRegressor model parameters\n",
    "params_rfr = {\n",
    "    'criterion' : 'squared_error', # The quality metric used for splitting. 'squared_error' indicates the use of MSE.\n",
    "    'min_weight_fraction_leaf' : 0.0, # Similar to 'min_samples_leaf', but based on the total sample weights.\n",
    "    'random_state' : 42,          # The random number generation to ensure reproducible results.\n",
    "    'max_leaf_nodes' : None,      # The maximum number of leaf nodes per tree. 'None' indicates no limit.\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model_rfr = RandomForestRegressor(**params_rfr)\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500], # Number of trees\n",
    "    'max_depth': [None, 3, 5, 7], # Maximum depth of each tree. 'None' indicates no depth limit.\n",
    "    'min_samples_split': [2, 3, 4, 5, 6], # Minimum number of samples required for a node to split.\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5] # Minimum number of samples required for a leaf node.\n",
    "\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for grid search and k-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_rfr,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Evaluation metric is negative MSE\n",
    "    cv=10,                             # 10-fold cross-validation\n",
    "    n_jobs=-1,                         # Parallel computing\n",
    "    verbose=1                          # Output detailed progress information\n",
    ")\n",
    "\n",
    "# Model training\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the optimal parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best RMSE score: \", (-grid_search.best_score_) ** 0.5)  # Restore RMSE\n",
    "\n",
    "# Train the model with optimal parameters\n",
    "best_model_rfr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and sort the feature importance of RandomForest\n",
    "rf_feature_importances = best_model_rfr.feature_importances_\n",
    "rf_sorted_indices = np.argsort(rf_feature_importances)[::-1]\n",
    "rf_sorted_features = X_train.columns[rf_sorted_indices]\n",
    "rf_sorted_importances = rf_feature_importances[rf_sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_model_rfr.feature_importances_\n",
    "\n",
    "# Construct a feature importance ranking\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Take the top 20 important features\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "# Adjust the font size\n",
    "plt.figure(figsize=(12, 8), dpi=1000)  \n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance', fontsize=14)  \n",
    "plt.ylabel('Feature', fontsize=14)  \n",
    "plt.title(f'Top {top_n} Feature Importance', fontsize=16)  \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)  \n",
    "plt.gca().invert_yaxis()  \n",
    "plt.savefig(\"1.png\", format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# XGBoost model parameters\n",
    "params_xgb = {\n",
    "    'booster': 'gbtree',              # Boosting method, here using Gradient Boosting Tree.\n",
    "    'objective': 'reg:squarederror',  # Loss function, here using squared error, which is suitable for regression tasks.\n",
    "    'max_leaves': 127,                # The number of leaf nodes per tree, which controls the model complexity.\n",
    "    'verbosity': 1,                   # The verbosity of XGBoost's output. 0 means no output, and 1 means outputting progress information.\n",
    "    'seed': 42,                       # Random seed, used to reproduce the model's results.\n",
    "    'nthread': -1,                    # The number of threads for parallel computation, where -1 indicates using all available CPU cores.\n",
    "    'colsample_bytree': 0.6,          # The proportion of features randomly selected for each tree, used to enhance the model's generalization ability.\n",
    "    'subsample': 0.7,                 # The proportion of samples randomly selected in each iteration, used to enhance the model's generalization ability.\n",
    "    'eval_metric': 'rmse'             # Evaluation metric, here using RMSE.\n",
    "}\n",
    "\n",
    "# nitialize the XGBoost model\n",
    "model_xgb = xgb.XGBRegressor(**params_xgb)\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Number of trees.\n",
    "    'max_depth': [3, 4, 5, 6, 7],               # Depth of the tree.\n",
    "    'learning_rate': [0.01, 0.02, 0.05, 0.1],   # Learning rate.\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for grid search and k-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Evaluation metric is negative mean squared error\n",
    "    cv=10,                             # 10-fold cross-validation\n",
    "    n_jobs=-1,                         # Parallel computing\n",
    "    verbose=1                          # Output detailed progress information\n",
    ")\n",
    "\n",
    "# Model training\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the optimal parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best RMSE score: \", (-grid_search.best_score_) ** 0.5)  # 还原RMSE\n",
    "\n",
    "# Train the model with optimal parameters\n",
    "best_model_xgboost = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and sort the feature importance of XGBoost\n",
    "xgb_feature_importances = best_model_xgboost.feature_importances_\n",
    "xgb_sorted_indices = np.argsort(xgb_feature_importances)[::-1]\n",
    "xgb_sorted_features = X_train.columns[xgb_sorted_indices]\n",
    "xgb_sorted_importances = xgb_feature_importances[xgb_sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importances = best_model_xgboost.feature_importances_\n",
    "\n",
    "# Construct a feature importance ranking\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Take the top 20 important features\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "# Adjust the font size\n",
    "plt.figure(figsize=(12, 8), dpi=1000)  \n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance', fontsize=14)  \n",
    "plt.ylabel('Feature', fontsize=14)  \n",
    "plt.title(f'Top {top_n} Feature Importance', fontsize=16)  \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)  \n",
    "plt.gca().invert_yaxis()  \n",
    "plt.savefig(\"1.png\", format='png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to store the feature importance of all models.\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"RandomForest_Feature\": rf_sorted_features,\n",
    "    \"RandomForest_Importance\": rf_sorted_importances,\n",
    "    \"XGBoost_Feature\": xgb_sorted_features,\n",
    "    \"XGBoost_Importance\": xgb_sorted_importances,\n",
    "    \"LightGBM_Feature\": lgb_sorted_features,\n",
    "    \"LightGBM_Importance\": lgb_sorted_importances,\n",
    "    \"CatBoost_Feature\": catboost_sorted_features,\n",
    "    \"CatBoost_Importance\": catboost_sorted_importances\n",
    "})\n",
    "feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sissopp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
